{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Causal Estimates with Limited Supervision\n",
    "In this notebook we:\n",
    "\n",
    "- Utilize a graph neural network to predict the effect of a marketing campaign to the user's consumption.\n",
    "\n",
    "- Compare our model with standard causal machine learning methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  (user, buys, product)={\n",
       "    edge_index=[2, 14543339],\n",
       "    treatment=[14543339],\n",
       "  },\n",
       "  user={\n",
       "    x=[180653, 7],\n",
       "    t=[180653],\n",
       "    y=[180653],\n",
       "  },\n",
       "  products={ num_products=40542 }\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.load(\"../data/retail/processed/data.pt\")[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def outcome_regression_loss_l1(t_true: torch.tensor,\n",
    "                               y_treatment_pred: torch.tensor, \n",
    "                               y_control_pred: torch.tensor, \n",
    "                               y_true: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Compute mse for treatment and control output layers using treatment vector for masking out the counterfactual predictions\n",
    "    \"\"\"\n",
    "    loss0 = torch.mean(((1. - t_true) * F.l1_loss(y_control_pred.squeeze(), y_true, reduction='none')) )\n",
    "    loss1 = torch.mean((t_true *  F.l1_loss(y_treatment_pred.squeeze(), y_true, reduction='none') ))\n",
    "\n",
    "    return (loss0 + loss1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def uplift_score(prediction: torch.tensor, \n",
    "                 treatment: torch.tensor, \n",
    "                 target: torch.tensor, \n",
    "                 rate=0.2) -> float:\n",
    "    \"\"\"\n",
    "    From https://ods.ai/competitions/x5-retailhero-uplift-modeling/data\n",
    "    Order the samples by the predicted uplift. \n",
    "    Calculate the average ground truth outcome of the top rate*100% of the treated and the control samples.\n",
    "    Subtract the above to get the uplift. \n",
    "    \"\"\"\n",
    "    order = np.argsort(-prediction)\n",
    "    treatment_n = int((treatment == 1).sum() * rate)\n",
    "    treatment_p = target[order][treatment[order] == 1][:treatment_n].mean()\n",
    "\n",
    "    control_n = int((treatment == 0).sum() * rate)\n",
    "    control_p = target[order][treatment[order] == 0][:control_n].mean()\n",
    "    score = treatment_p - control_p\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n_hidden = 16\n",
    "lr = 0.01\n",
    "l2_reg = 5e-4 \n",
    "dropout = 0.2\n",
    "\n",
    "no_layers = 1 \n",
    "out_channels = 1\n",
    "num_epochs = 300\n",
    "\n",
    "seed = 0\n",
    "k = 10\n",
    "validation_fraction = 5\n",
    "patience = 50\n",
    "\n",
    "criterion = outcome_regression_loss_l1\n",
    "\n",
    "\n",
    "torch.manual_seed(seed) \n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "xp = torch.eye(data['products']['num_products']).to(device)\n",
    "xu = data['user']['x'].to(device)\n",
    "treatment = data['user']['t'].to(device)\n",
    "outcome = data['user']['y'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=abs(k), shuffle=True, random_state=seed)\n",
    "\n",
    "for train_indices, test_indices in kf.split(xu):\n",
    "    test_indices, train_indices = train_indices, test_indices\n",
    "    break \n",
    "\n",
    "val_indices = train_indices[:int(len(train_indices)/validation_fraction)]\n",
    "train_indices = train_indices[int(len(train_indices)/validation_fraction):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the graph before the treatment and ONLY the edges of the the train nodes (i.e. after the treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.isin(data['user','buys','product']['edge_index'][0, :], torch.tensor(train_indices) )\n",
    "edge_index_up_current = data['user','buys','product']['edge_index'][ : , (~data['user','buys','product']['treatment']) | (mask) ]\n",
    "\n",
    "\n",
    "#interaction to adjecency matrix\n",
    "edge_index_up_current[1] = edge_index_up_current[1]+ xu.shape[0]\n",
    "\n",
    "edge_index_up_current = torch.cat([edge_index_up_current,edge_index_up_current.flip(dims=[0])],dim=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = data['user','buys','product']['edge_index'][ : , (~data['user','buys','product']['treatment']) | (mask) ]\n",
    "sparse_matrix_edge_index = torch.sparse_coo_tensor(\n",
    "            edge_index,\n",
    "            torch.ones(edge_index.shape[1]),\n",
    "            (xu.shape[0], xp.shape[0]),\n",
    "            dtype=torch.float\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_treatment_feature(x, train_indices, treatment):\n",
    "    t_hat = torch.zeros(x.size(0), 2, dtype=torch.float)\n",
    "    t_hat[train_indices,treatment.type(torch.LongTensor)[train_indices]]=1\n",
    "    return t_hat\n",
    "\n",
    "new_feature = make_treatment_feature(xu, train_indices, treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/nn8th6r16cq6sytrzw1nw_9w0000gn/T/ipykernel_55945/3973650620.py:1: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  product_treatment_matrix = torch.sparse.mm(sparse_matrix_edge_index.t(), new_feature.to_sparse())\n"
     ]
    }
   ],
   "source": [
    "product_treatment_matrix = torch.sparse.mm(sparse_matrix_edge_index.t(), new_feature.to_sparse())\n",
    "user_treatment_matrix = torch.sparse.mm(sparse_matrix_edge_index, product_treatment_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 80.,  78.],\n",
       "        [ 90., 128.],\n",
       "        [ 45.,  52.],\n",
       "        ...,\n",
       "        [  0.,   1.],\n",
       "        [  0.,   0.],\n",
       "        [  0.,   0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_treatment_matrix.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24066., 24263.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_treatment_matrix.to_dense()[9,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user_matrix = torch.sparse.mm(sparse_matrix_edge_index, sparse_matrix_edge_index.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_user_treatment_matrix = torch.sparse.mm(user_user_matrix, new_feature.to_sparse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import torch_geometric as pyg\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "from typing import Callable\n",
    "import torch_geometric as pyg\n",
    "from torch.optim import Optimizer\n",
    "\n",
    " \n",
    "def train(mask: torch.tensor,  #np.ndarray, \n",
    "          model:torch.nn.Module, \n",
    "          xu: torch.tensor, \n",
    "          xp: torch.tensor, \n",
    "          edge_index: torch.tensor, \n",
    "          treatment: torch.tensor, \n",
    "          outcome: torch.tensor,\n",
    "          optimizer: Optimizer, \n",
    "          criterion: Callable[[torch.tensor, torch.tensor, torch.tensor, torch.tensor], torch.tensor] ) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    pred_t, pred_c, hidden_treatment, hidden_control = model(xu, xp, edge_index)\n",
    "    \n",
    "    \n",
    "    pred_t = model(torch.cat[xu,1], xp, edge_index)\n",
    "    pred_c = model(torch.cat[xu,0], xp, edge_index)\n",
    "    loss = criterion(treatment[mask], pred_t[mask], pred_c[mask], outcome[mask])\n",
    "    \n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def test(mask: torch.tensor,  #np.ndarray, \n",
    "          model:torch.nn.Module, \n",
    "          xu: torch.tensor, \n",
    "          xp: torch.tensor, \n",
    "          edge_index: torch.tensor, \n",
    "          treatment: torch.tensor, \n",
    "          outcome: torch.tensor,\n",
    "          criterion: Callable[[torch.tensor, torch.tensor, torch.tensor, torch.tensor], torch.tensor] ) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Tests the model. \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred_t, pred_c, hidden_treatment, hidden_control = model(xu, xp, edge_index)\n",
    "    loss = criterion(treatment[mask], pred_t[mask], pred_c[mask], outcome[mask])\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     9,      9,      9,  ..., 189599, 191531, 199951],\n",
       "        [180660, 180674, 180688,  ..., 180632, 180632, 180632]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index_up_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "class BipartiteSAGE2mod_interf(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat:int, nproduct:int , hidden_channels:int , out_channels: int, num_layers:int, dropout_rate:float =0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        self.user_embed = nn.Linear(nfeat, hidden_channels )\n",
    "        self.item_embed =  nn.Linear(nproduct, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(SAGEConv((-1,-1), hidden_channels))\n",
    "            \n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.hidden_common1 = nn.Linear(hidden_channels + num_layers*hidden_channels, hidden_channels)\n",
    "        self.hidden_common2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.hidden_control = nn.Linear(hidden_channels, int(hidden_channels/2))\n",
    "        self.hidden_treatment = nn.Linear(hidden_channels, int(hidden_channels/2))\n",
    "\n",
    "        self.out = nn.Linear( int(hidden_channels/2), out_channels)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, xu: torch.tensor, xp:torch.tensor, edge_index:torch._tensor):\n",
    "        out = [] \n",
    "        xu = self.user_embed(xu)\n",
    "        xp = self.item_embed(xp)\n",
    "\n",
    "        out.append(xu)\n",
    "\n",
    "        embeddings = torch.cat((xu,xp), dim=0) \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            embeddings = self.activation(self.convs[i](embeddings, edge_index))\n",
    "            \n",
    "            out.append(embeddings[:xu.shape[0]])            \n",
    "        \n",
    "        out = torch.cat( out, dim=1)\n",
    "        \n",
    "        hidden = self.dropout(self.activation(self.hidden_common1(out)))\n",
    "        hidden = self.dropout(self.activation(self.hidden_common2(hidden)))        \n",
    "        \n",
    "        out = self.activation(self.out(hidden))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "class BipartiteSAGE2mod(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat:int, nproduct:int , hidden_channels:int , out_channels: int, num_layers:int, dropout_rate:float =0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        self.user_embed = nn.Linear(nfeat, hidden_channels )\n",
    "        self.item_embed =  nn.Linear(nproduct, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(SAGEConv((-1,-1), hidden_channels))\n",
    "            \n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.hidden_common1 = nn.Linear(hidden_channels + num_layers*hidden_channels, hidden_channels)\n",
    "        self.hidden_common2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.hidden_control = nn.Linear(hidden_channels, int(hidden_channels/2))\n",
    "        self.hidden_treatment = nn.Linear(hidden_channels, int(hidden_channels/2))\n",
    "\n",
    "        self.out_control = nn.Linear( int(hidden_channels/2), out_channels)\n",
    "        self.out_treatment = nn.Linear( int(hidden_channels/2), out_channels)\n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, xu: torch.tensor, xp:torch.tensor, edge_index:torch._tensor):\n",
    "        out = [] \n",
    "        xu = self.user_embed(xu)\n",
    "        xp = self.item_embed(xp)\n",
    "\n",
    "        out.append(xu)\n",
    "\n",
    "        embeddings = torch.cat((xu,xp), dim=0) \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            embeddings = self.activation(self.convs[i](embeddings, edge_index))\n",
    "            \n",
    "            out.append(embeddings[:xu.shape[0]])            \n",
    "        \n",
    "        out = torch.cat( out, dim=1)\n",
    "        \n",
    "        hidden = self.dropout(self.activation(self.hidden_common1(out)))\n",
    "        hidden = self.dropout(self.activation(self.hidden_common2(hidden)))\n",
    "        \n",
    "        hidden_1t0 = self.dropout(self.activation(self.hidden_control(hidden)))\n",
    "        hidden_1t1 = self.dropout(self.activation(self.hidden_treatment(hidden)))\n",
    "\n",
    "        out_2t0 = self.activation(self.out_control(hidden_1t0))\n",
    "        out_2t1 = self.activation(self.out_treatment(hidden_1t1))\n",
    "        \n",
    "        return out_2t1, out_2t0, hidden_1t1, hidden_1t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model_file = \"../models/bipartite_sage\"\n",
    "\n",
    "    early_stopping = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    print_per_epoch = 50\n",
    "    \n",
    "            \n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 16, 64)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-1)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 128)\n",
    "    \n",
    "     \n",
    "    model = BipartiteSAGE2mod(xu.shape[1], xp.shape[1] , hidden_channels, out_channels, num_layers, dropout_rate).to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=lr, weight_decay = l2_reg)\n",
    "\n",
    "    # init params\n",
    "    out = model(xu, xp, edge_index_up_current)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(train_indices,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        mean_val_loss = 0\n",
    "        for train_batch_index in data_loader:\n",
    "            train_loss = train(train_batch_index, model, xu, xp, edge_index_up_current, treatment, outcome, optimizer, criterion)\n",
    "            val_loss = test(val_indices, model, xu, xp, edge_index_up_current, treatment, outcome, criterion)\n",
    "            \n",
    "            mean_val_loss += val_loss/len(data_loader)\n",
    "            train_losses.append(float(train_loss.item())) \n",
    "            \n",
    "        val_losses.append(float(mean_val_loss.item()))\n",
    "\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            early_stopping=0\n",
    "            best_val_loss = mean_val_loss\n",
    "            torch.save(model, model_file + f\"_{trial.number}.pt\")\n",
    "        else:\n",
    "            early_stopping += 1\n",
    "            if early_stopping > patience:\n",
    "                print(\"early stopping..\")\n",
    "                break\n",
    "\n",
    "        if epoch%print_per_epoch==0:\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Val loss: {mean_val_loss:.4f}') #, Test: {test_acc:.4f}'\n",
    "        \n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-09 10:21:30,336] A new study created in memory with name: no-name-e5bd9d3f-b5d0-49cc-b446-aa8f99553ddc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 216.4955, Val loss: 220.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-09 10:22:18,398] Trial 0 finished with value: 220.30877685546875 and parameters: {'hidden_channels': 21, 'dropout_rate': 0.46293094056179407, 'num_layers': 3, 'lr': 0.017118077953755927}. Best is trial 0 with value: 220.30877685546875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 220.4789, Val loss: 220.2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-09 10:23:02,334] Trial 1 finished with value: 220.21966552734375 and parameters: {'hidden_channels': 26, 'dropout_rate': 0.4785508637733481, 'num_layers': 1, 'lr': 0.0020037920926335795}. Best is trial 1 with value: 220.21966552734375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 214.7193, Val loss: 191.3461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-09 10:23:53,591] Trial 2 finished with value: 151.9998779296875 and parameters: {'hidden_channels': 27, 'dropout_rate': 0.29248676547613817, 'num_layers': 3, 'lr': 0.08066003013340728}. Best is trial 2 with value: 151.9998779296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Loss: 173.6843, Val loss: 501.2159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-07-09 10:24:53,226] Trial 3 finished with value: 218.76510620117188 and parameters: {'hidden_channels': 52, 'dropout_rate': 0.20264813012825394, 'num_layers': 2, 'lr': 0.07578702306377659}. Best is trial 2 with value: 151.9998779296875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  4\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  4\n",
      "Best trial:\n",
      "  Value:  151.9998779296875\n",
      "  Params: \n",
      "    hidden_channels: 27\n",
      "    dropout_rate: 0.29248676547613817\n",
      "    num_layers: 3\n",
      "    lr: 0.08066003013340728\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective,\n",
    "               n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_model = study.best_trial\n",
    "\n",
    "model_file = \"../models/bipartite_sage\" + f\"_{best_model.number}.pt\"\n",
    "\n",
    "print(\"  Value: \", best_model.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_model.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse 154.8895 with avg abs value 445.1011657714844\n",
      "up40 6.7227\n",
      "up20 7.5219\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_file).to(device)\n",
    "model.eval()\n",
    "\n",
    "mask = test_indices\n",
    "pred_t, pred_c, hidden_treatment, hidden_control = model(xu, xp, edge_index_up_current)\n",
    "\n",
    "test_loss = criterion(treatment[mask], pred_t[mask], pred_c[mask], outcome[mask])\n",
    "\n",
    "treatment_test = treatment[test_indices].detach().cpu().numpy()\n",
    "outcome_test = outcome[test_indices].detach().cpu().numpy()\n",
    "pred_t = pred_t.detach().cpu().numpy()\n",
    "pred_c = pred_c.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "uplift = pred_t[test_indices] - pred_c[test_indices]\n",
    "uplift = uplift.squeeze()\n",
    "\n",
    "#mse = (uplift.mean() - (outcome_test[treatment_test==1].mean() - outcome_test[treatment_test==0].mean()))**2\n",
    "print(f'mse {test_loss:.4f} with avg abs value {torch.mean(torch.abs(outcome[mask]))}')\n",
    "up40 = uplift_score(uplift, treatment_test, outcome_test,0.4)\n",
    "print(f'up40 {up40:.4f}')\n",
    "up20 = uplift_score(uplift, treatment_test, outcome_test,0.2)\n",
    "print(f'up20 {up20:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import duecredit due to No module named 'duecredit'\n"
     ]
    }
   ],
   "source": [
    "from causalml.inference.meta import  BaseXRegressor, BaseTRegressor\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from xgboost import XGBRegressor\n",
    "from causalml.inference.tree.causal.causaltree import CausalTreeRegressor\n",
    "\n",
    "train_indices = np.hstack([train_indices, val_indices])\n",
    "outcome = outcome.cpu().numpy()\n",
    "xu = xu.cpu().numpy()\n",
    "treatment = treatment.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-learner up40: 0.5721 , up20: 1.7642\n"
     ]
    }
   ],
   "source": [
    "learner = BaseTRegressor(learner = XGBRegressor())\n",
    "\n",
    "learner.fit(X= xu[train_indices], y=outcome[train_indices], treatment= treatment[train_indices] )  \n",
    "uplift=learner.predict(X = xu[train_indices], treatment = treatment[test_indices]).squeeze()\n",
    "\n",
    "uplift=learner.predict(X = xu[test_indices], treatment= treatment[test_indices]).squeeze()\n",
    "\n",
    "up40 = uplift_score(uplift, np.hstack([treatment[train_indices] ,treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.4)\n",
    "up20 = uplift_score(uplift, np.hstack([treatment[train_indices],treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.2)\n",
    "\n",
    "print(f'T-learner up40: {up40:.4f} , up20: {up20:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n",
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-learner up40: -1.6282 , up20: -2.2200\n"
     ]
    }
   ],
   "source": [
    "propensity_model = ElasticNetPropensityModel()\n",
    "\n",
    "propensity_model.fit(X=xu[train_indices], y = treatment[train_indices])\n",
    "p_train = propensity_model.predict(X=xu[train_indices])\n",
    "p_test = propensity_model.predict(X=xu[test_indices])\n",
    "\n",
    "learner = BaseXRegressor(learner = XGBRegressor())\n",
    "\n",
    "learner.fit(X= xu[train_indices], y=outcome[train_indices], treatment= treatment[train_indices], p=p_train )  \n",
    "\n",
    "uplift=learner.predict(X = xu[test_indices], treatment= treatment[test_indices], p=p_test).squeeze()\n",
    "\n",
    "up40 = uplift_score(uplift, np.hstack([treatment[train_indices] ,treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.4)\n",
    "up20 = uplift_score(uplift, np.hstack([treatment[train_indices],treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.2)\n",
    "\n",
    "print(f'X-learner up40: {up40:.4f} , up20: {up20:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree up40: -1.6412 , up20: 5.2740\n"
     ]
    }
   ],
   "source": [
    "learner = CausalTreeRegressor(control_name=\"0\")\n",
    "X_train = np.hstack(( treatment[train_indices].reshape(-1, 1), xu[train_indices]))\n",
    "X_test = np.hstack((treatment[test_indices].reshape(-1, 1),  xu[test_indices]))\n",
    "learner.fit( X = X_train, treatment = treatment[train_indices].astype(str), y=outcome[train_indices])\n",
    "uplift = learner.predict( X = X_test).squeeze()\n",
    "\n",
    "up40 = uplift_score(uplift, np.hstack([treatment[train_indices] ,treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.4)\n",
    "up20 = uplift_score(uplift, np.hstack([treatment[train_indices],treatment[test_indices]]), np.hstack([outcome[train_indices],outcome[test_indices]]), rate=0.2)\n",
    "\n",
    "print(f'Tree up40: {up40:.4f} , up20: {up20:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
